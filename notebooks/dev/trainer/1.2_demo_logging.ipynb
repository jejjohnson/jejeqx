{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Demo Regression\n",
    "\n",
    "In this notebook, we will look showcase how to implement a JAX trainer for research purposes. We will use the documentation from the [uvadlc notebooks](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/guide4/Research_Projects_with_JAX.html) and adapt this to the libraries I would like to use:\n",
    "\n",
    "* jax\n",
    "* equinox\n",
    "* optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning\n",
    "import sys, os\n",
    "from pyprojroot import here\n",
    "\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".home\"])\n",
    "nn_model = \"/Users/eman/code_projects/eqx-nerf\"\n",
    "\n",
    "# append to path\n",
    "sys.path.append(str(root))\n",
    "sys.path.append(str(nn_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# JAX/Flax\n",
    "# If you run this code on Colab, remember to install flax and optax\n",
    "# !pip install --quiet --upgrade flax optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "import equinox as eqx\n",
    "\n",
    "# PyTorch for data loading\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Logging with Tensorboard or Weights and Biases\n",
    "# If you run this code on Colab, remember to install pytorch_lightning\n",
    "# !pip install --quiet --upgrade pytorch_lightning\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context=\"talk\", font_scale=0.7)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    return np.sin(x * 10.0)\n",
    "\n",
    "\n",
    "class RegressionDataset(data.Dataset):\n",
    "    def __init__(self, num_points, seed):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.x = rng.uniform(low=-1.0, high=1.0, size=num_points)\n",
    "        self.y = target_function(self.x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx : idx + 1], self.y[idx : idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_set = RegressionDataset(num_points=1000, seed=42)\n",
    "val_set = RegressionDataset(num_points=200, seed=43)\n",
    "test_set = RegressionDataset(num_points=500, seed=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)\n",
    "plt.scatter(\n",
    "    train_set.x, train_set.y, color=\"C1\", marker=\"x\", alpha=0.5, label=\"Training set\"\n",
    ")\n",
    "plt.plot(x, target_function(x), linewidth=3.0, label=\"Ground Truth Function\")\n",
    "plt.legend()\n",
    "plt.title(\"Regression function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from torch.utils.data import DataLoader\n",
    "from eqx_trainer import numpy_collate\n",
    "\n",
    "\n",
    "class NumpyLoader(DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        collate_fn=numpy_collate,\n",
    "        persistent_workers=False,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class RegressionDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_train: int = 1000,\n",
    "        num_valid: int = 200,\n",
    "        num_test: int = 500,\n",
    "        seed=42,\n",
    "        num_workers: int = 0,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_train = num_train\n",
    "        self.num_valid = num_valid\n",
    "        self.num_test = num_test\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        self.train_set = RegressionDataset(num_points=self.num_train, seed=42)\n",
    "        self.val_set = RegressionDataset(num_points=self.num_valid, seed=43)\n",
    "        self.test_set = RegressionDataset(num_points=self.num_test, seed=44)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.val_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dm = RegressionDataModule(num_train=1_000, num_valid=200, num_test=500, batch_size=128)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for ibatch in tqdm(dm.train_dataloader()):\n",
    "    break\n",
    "\n",
    "print(ibatch[0].shape, ibatch[1].shape, type(ibatch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    width_size: int = 32\n",
    "    depth: int = 2\n",
    "    lr: float = 5e-3\n",
    "    num_epochs: int = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "from equinox.nn.linear import Identity\n",
    "from eqx_nerf import SirenNet\n",
    "import jax.random as jrandom\n",
    "\n",
    "\n",
    "def init_model(width_size: int = 32, depth: int = 2):\n",
    "    model = eqx.nn.MLP(\n",
    "        in_size=1,\n",
    "        out_size=1,\n",
    "        width_size=width_size,\n",
    "        depth=depth,\n",
    "        # activation=eqx.nn.Lambda(jax.nn.silu),\n",
    "        activation=eqx.nn.activations.PReLU(),\n",
    "        final_activation=Identity(),\n",
    "        key=jrandom.PRNGKey(123),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "from equinox.nn.linear import Identity\n",
    "from eqx_nerf import SirenNet\n",
    "import jax.random as jrandom\n",
    "\n",
    "model = init_model(width_size=params.width_size, depth=params.depth)\n",
    "\n",
    "# check output of models\n",
    "out = jax.vmap(model)(ibatch[0])\n",
    "\n",
    "assert out.shape == ibatch[0].shape\n",
    "eqx.tree_pprint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Optimizer (+ Learning Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "def init_optimizer(\n",
    "    num_steps_per_epoch: int, lr: float = 4e-4, num_epochs: int = 500, **kwargs\n",
    "):\n",
    "    # optimizer = optax.adamw(learning_rate=learning_rate)\n",
    "    lr_scheduler = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=kwargs.get(\"start_lr\", 0.0),\n",
    "        peak_value=lr,\n",
    "        warmup_steps=kwargs.get(\"warmup_steps\", 100),\n",
    "        decay_steps=int(num_epochs * num_steps_per_epoch),\n",
    "        end_value=kwargs.get(\"eta\", 0.01) * lr,\n",
    "    )\n",
    "\n",
    "    return optax.adamw(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = init_optimizer(\n",
    "    num_epochs=params.num_epochs,\n",
    "    lr=params.lr,\n",
    "    num_steps_per_epoch=len(dm.train_dataloader()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eqx_trainer._src.utils import dataclass_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = dataclass_to_dict(params)\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = list()\n",
    "\n",
    "log_dir = \"/Users/eman/code_projects/logs\"\n",
    "wandb_logger = WandbLogger(\n",
    "    mode=\"online\", save_dir=log_dir, entity=\"ige\", project=\"jax4eo\", config=config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Trainer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from eqx_trainer import TrainerModule\n",
    "from eqx_trainer._src.callbacks import wandb_model_artifact\n",
    "\n",
    "\n",
    "class RegressorTrainer(TrainerModule):\n",
    "    def __init__(self, model, optimizer, pl_logger, **kwargs):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            pl_logger=pl_logger,\n",
    "            log_dir=pl_logger.experiment.dir,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def create_functions(self):\n",
    "        @eqx.filter_value_and_grad\n",
    "        def mse_loss(model, batch):\n",
    "            x, y = batch\n",
    "            pred = jax.vmap(model)(x)\n",
    "            loss = jnp.mean((y - pred) ** 2)\n",
    "            return loss\n",
    "\n",
    "        def train_step(state, batch):\n",
    "            loss, grads = mse_loss(state.params, batch)\n",
    "            state = state.update_state(state, grads)\n",
    "            metrics = {\"loss\": loss}\n",
    "            return state, loss, metrics\n",
    "\n",
    "        #         def train_step(model, opt_state, batch):\n",
    "\n",
    "        #             loss, grads = mse_loss(model, batch)\n",
    "        #             model, opt_state = self.apply_updates(model, opt_state, grads)\n",
    "        #             metrics = {\"loss\": loss}\n",
    "        #             return model, opt_state, loss, metrics\n",
    "\n",
    "        def eval_step(model, batch):\n",
    "            loss, _ = mse_loss(model, batch)\n",
    "            return {\"loss\": loss}\n",
    "\n",
    "        return train_step, eval_step\n",
    "\n",
    "    def on_training_end(\n",
    "        self,\n",
    "    ):\n",
    "        if self.pl_logger:\n",
    "            save_dir = Path(self.log_dir).joinpath(self.save_name)\n",
    "            self.save_model(save_dir)\n",
    "            wandb_model_artifact(self)\n",
    "            self.pl_logger.finalize(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "debug = False\n",
    "enable_progress_bar = False\n",
    "\n",
    "trainer = RegressorTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    seed=seed,\n",
    "    debug=debug,\n",
    "    enable_progress_bar=enable_progress_bar,\n",
    "    pl_logger=wandb_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "metrics = trainer.train_model(dm, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# trainer.save_model(\"checkpoint_model.ckpt\")\n",
    "# trainer.save_state(\"checkpoint_state.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)[:, None]\n",
    "y_pred = jax.vmap(trainer.model)(ibatch[0])\n",
    "plt.scatter(ibatch[0], y_pred, label=\"Prediction\")\n",
    "plt.plot(x, target_function(x), \"--\", label=\"GT\")\n",
    "plt.title(\"Function regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)[:, None]\n",
    "y_pred = trainer.model_batch(x)\n",
    "plt.plot(x, y_pred, label=\"Prediction\")\n",
    "plt.plot(x, target_function(x), \"--\", label=\"GT\")\n",
    "plt.title(\"Function regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Loading CheckPoints\n",
    "\n",
    "\n",
    "* Pre-Trained Models\n",
    "* Training \"More\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    width_size: int = 32\n",
    "    depth: int = 2\n",
    "    lr: float = 5e-3\n",
    "    num_epochs: int = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params()\n",
    "\n",
    "config_dict = dataclass_to_dict(params)\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eqx_trainer._src.utils.wandb import (\n",
    "    load_wandb_run_config,\n",
    "    download_wandb_artifact_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "\n",
    "We can look at a previous run on weights and biases!\n",
    "```\n",
    "ige/jax4eo/p017leep\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"ige\"\n",
    "project = \"jax4eo\"\n",
    "id = \"p017leep\"\n",
    "\n",
    "# load old config\n",
    "old_config = load_wandb_run_config(entity=entity, project=project, id=id)\n",
    "\n",
    "old_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggers = list()\n",
    "\n",
    "log_dir = \"/Users/eman/code_projects/logs\"\n",
    "wandb_logger = WandbLogger(\n",
    "    mode=\"online\", save_dir=log_dir, entity=\"ige\", project=\"jax4eo\", config=config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.experiment.config.update(\n",
    "    {\"width_size\": old_config[\"width_size\"], \"depth\": old_config[\"depth\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb artifact get ige/jax4eo/experiments-ckpts:v21 --root artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"ige\"\n",
    "project = \"jax4eo\"\n",
    "id = \"p017leep\"\n",
    "reference = \"experiments-ckpts:v17\"\n",
    "ckpt_name = \"checkpoint_model\"\n",
    "\n",
    "# get checkpoint filename\n",
    "ckpt_file = download_wandb_artifact_model(\n",
    "    entity, project, reference, \"checkpoint_model\"\n",
    ")\n",
    "\n",
    "ckpt_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model(width_size=old_config[\"width_size\"], depth=old_config[\"depth\"])\n",
    "\n",
    "\n",
    "optimizer = init_optimizer(\n",
    "    num_epochs=config_dict[\"num_epochs\"],\n",
    "    lr=config_dict[\"lr\"],\n",
    "    num_steps_per_epoch=len(dm.train_dataloader()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "debug = False\n",
    "enable_progress_bar = False\n",
    "\n",
    "trainer = RegressorTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    seed=seed,\n",
    "    debug=debug,\n",
    "    enable_progress_bar=enable_progress_bar,\n",
    "    pl_logger=wandb_logger,\n",
    ")\n",
    "\n",
    "trainer.load_model(ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "metrics = trainer.train_model(dm, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"checkpoint_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)[:, None]\n",
    "y_pred_more = trainer.model_batch(x)\n",
    "plt.plot(x, y_pred, label=\"Prediction\")\n",
    "plt.plot(x, y_pred_more, label=\"Prediction (More)\")\n",
    "plt.plot(x, target_function(x), \"--\", label=\"GT\")\n",
    "plt.title(\"Function regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jax_eo_py310]",
   "language": "python",
   "name": "conda-env-jax_eo_py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
