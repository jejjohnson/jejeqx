{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Regression\n",
    "\n",
    "In this notebook, we will look showcase how to implement a JAX trainer for research purposes. We will use the documentation from the [uvadlc notebooks](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/guide4/Research_Projects_with_JAX.html) and adapt this to the libraries I would like to use:\n",
    "* serket\n",
    "* optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning\n",
    "import sys, os\n",
    "from pyprojroot import here\n",
    "\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".home\"])\n",
    "\n",
    "# append to path\n",
    "sys.path.append(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "# JAX/Flax\n",
    "# If you run this code on Colab, remember to install flax and optax\n",
    "# !pip install --quiet --upgrade flax optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax import linen as nn\n",
    "\n",
    "# from flax.training import train_state, checkpoints\n",
    "import optax\n",
    "\n",
    "# PyTorch for data loading\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Logging with Tensorboard or Weights and Biases\n",
    "# If you run this code on Colab, remember to install pytorch_lightning\n",
    "# !pip install --quiet --upgrade pytorch_lightning\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context=\"talk\", font_scale=0.7)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    return np.sin(x * 3.0)\n",
    "\n",
    "\n",
    "class RegressionDataset(data.Dataset):\n",
    "    def __init__(self, num_points, seed):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.x = rng.uniform(low=-2.0, high=2.0, size=num_points)\n",
    "        self.y = target_function(self.x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx : idx + 1], self.y[idx : idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_set = RegressionDataset(num_points=1000, seed=42)\n",
    "val_set = RegressionDataset(num_points=200, seed=43)\n",
    "test_set = RegressionDataset(num_points=500, seed=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 1000)\n",
    "plt.scatter(\n",
    "    train_set.x, train_set.y, color=\"C1\", marker=\"x\", alpha=0.5, label=\"Training set\"\n",
    ")\n",
    "plt.plot(x, target_function(x), linewidth=3.0, label=\"Ground Truth Function\")\n",
    "plt.legend()\n",
    "plt.title(\"Regression function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from mylib._src.datamodules.base import NumpyLoader, numpy_collate\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class RegressionDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_train: int = 1000,\n",
    "        num_valid: int = 200,\n",
    "        num_test: int = 500,\n",
    "        seed=42,\n",
    "        num_workers: int = 0,\n",
    "        batch_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_train = num_train\n",
    "        self.num_valid = num_valid\n",
    "        self.num_test = num_test\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        self.train_set = RegressionDataset(num_points=self.num_train, seed=42)\n",
    "        self.val_set = RegressionDataset(num_points=self.num_valid, seed=43)\n",
    "        self.test_set = RegressionDataset(num_points=self.num_test, seed=44)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.val_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dm = RegressionDataModule(num_train=1_000, num_valid=200, num_test=500, batch_size=128)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for ibatch in tqdm(dm.train_dataloader()):\n",
    "    break\n",
    "\n",
    "print(ibatch[0].shape, ibatch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from mylib._src.trainers.trainstate import TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "from mylib._src.nets.nerfs.siren import SirenNet\n",
    "import jax.random as jrandom\n",
    "\n",
    "\n",
    "model = SirenNet(\n",
    "    in_dim=1, out_dim=1, hidden_dim=128, n_hidden=3, key=jrandom.PRNGKey(123)\n",
    ")\n",
    "\n",
    "# class Swish(eqx.Module):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#\n",
    "#     def __call__(self, x, *, key=None):\n",
    "#         return jax.nn.swish(x)\n",
    "#\n",
    "# model = eqx.nn.Sequential(\n",
    "#     [eqx.nn.Linear(in_features=1, out_features=128, key=jrandom.PRNGKey(123)),\n",
    "#      Swish(),\n",
    "#      eqx.nn.Linear(in_features=128, out_features=128, key=jrandom.PRNGKey(123)),\n",
    "#      Swish(),\n",
    "#      eqx.nn.Linear(in_features=128, out_features=1, key=jrandom.PRNGKey(123))\n",
    "#      ]\n",
    "# )\n",
    "\n",
    "# model = eqx.nn.MLP(\n",
    "#     in_size=1, out_size=1, width_size=128, depth=2,\n",
    "#     key=jrandom.PRNGKey(123)\n",
    "# )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 4e-3\n",
    "warmup_steps = 10\n",
    "num_epochs = 100\n",
    "num_steps_per_epoch = len(dm.train_dataloader())\n",
    "eta = 0.01\n",
    "start_lr = 0.0\n",
    "\n",
    "optimizer = optax.adamw(learning_rate=learning_rate)\n",
    "lr_scheduler = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=start_lr,\n",
    "    peak_value=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_steps=int(num_epochs * num_steps_per_epoch),\n",
    "    end_value=eta * learning_rate,\n",
    ")\n",
    "clip = optax.clip_by_global_norm(2.0)\n",
    "optimizer = optax.chain(clip, optax.adamw(lr_scheduler))\n",
    "# optimizer.init(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from mylib._src.trainers.base import TrainerModule\n",
    "\n",
    "\n",
    "class RegressorTrainer(TrainerModule):\n",
    "    def __init__(self, model, optimizer, input_init, **kwargs):\n",
    "        super().__init__(\n",
    "            model=model, optimizer=optimizer, input_init=input_init, **kwargs\n",
    "        )\n",
    "\n",
    "    def create_functions(self):\n",
    "        @eqx.filter_value_and_grad\n",
    "        def mse_loss(model, batch):\n",
    "            x, y = batch\n",
    "            pred = jax.vmap(model)(x)\n",
    "            loss = jnp.mean((y - pred) ** 2)\n",
    "            return loss\n",
    "\n",
    "        def train_step(model, batch, optim, opt_state):\n",
    "            loss, grads = mse_loss(model, batch)\n",
    "            updates, opt_state = optim.update(grads, opt_state)\n",
    "            model = eqx.apply_updates(model, updates)\n",
    "            metrics = {\"loss\": loss}\n",
    "            return loss, metrics, model, optim, opt_state\n",
    "\n",
    "        def eval_step(model, batch):\n",
    "            loss, _ = mse_loss(model, batch)\n",
    "            return {\"loss\": loss}\n",
    "\n",
    "        return train_step, eval_step\n",
    "\n",
    "    def init_optimizer(self, num_epochs: int, num_steps_per_epoch):\n",
    "        learning_rate = 4e-3\n",
    "\n",
    "        opt_class = optax.adam\n",
    "\n",
    "        lr_scheduler = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=learning_rate,\n",
    "            warmup_steps=10,\n",
    "            decay_steps=int(num_epochs * num_steps_per_epoch),\n",
    "            end_value=0.01 * learning_rate,\n",
    "        )\n",
    "\n",
    "        optimizer = optax.chain(optax.clip_by_global_norm(1.0), opt_class(lr_scheduler))\n",
    "        return optimizer\n",
    "\n",
    "    def init_logger(self, logger_params=None):\n",
    "        self.logger = WandbLogger(\n",
    "            mode=\"disabled\", save_dir=self.log_dir, entity=\"ige\", project=\"jax4eo\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trainer = RegressorTrainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    input_init=ibatch[0],\n",
    "    seed=123,\n",
    "    debug=False,\n",
    "    enable_progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# trainer.load_model(\"./checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "metrics = trainer.train_model(dm, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 1000)[:, None]\n",
    "y_pred = jax.vmap(trainer.model)(ibatch[0])\n",
    "plt.scatter(ibatch[0], y_pred, label=\"Prediction\")\n",
    "plt.plot(x, target_function(x), \"--\", label=\"GT\")\n",
    "plt.title(\"Function regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 1000)[:, None]\n",
    "y_pred = jax.vmap(trainer.model)(x)\n",
    "plt.plot(x, y_pred, label=\"Prediction\")\n",
    "plt.plot(x, target_function(x), \"--\", label=\"GT\")\n",
    "plt.title(\"Function regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.us-east-1.wasabisys.com/melody/osse_data/ref/NATL60-CJM165_GULFSTREAM_sst_y2013.1y.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "ds = xr.open_dataset(\n",
    "    \"NATL60-CJM165_GULFSTREAM_sst_y2013.1y.nc\", decode_times=True\n",
    ")  # .assign_coords(time=lambda ds: pd.to_datetime(ds.time))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=-1.1, vmax=1.1)\n",
    "cmap = cm.RdBu_r\n",
    "\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SineCosCycles:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, x):\n",
    "        data = x[\"time\"]\n",
    "        data = data.dt.day_of_year\n",
    "        data = np.concatenate(\n",
    "            [\n",
    "                np.sin(data[\"doy\"] / 360 * 2 * np.pi),\n",
    "                np.cos(data[\"doy\"] / 360 * 2 * np.pi),\n",
    "            ]\n",
    "        )\n",
    "        x[\"time\"] = data\n",
    "        return x\n",
    "\n",
    "\n",
    "class SSTDataset(data.Dataset):\n",
    "    def __init__(self, time_slice, variable, seed):\n",
    "        super().__init__()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        ds = xr.open_dataset(\n",
    "            \"NATL60-CJM165_GULFSTREAM_sst_y2013.1y.nc\", decode_times=False\n",
    "        ).assign_coords(time=lambda ds: pd.to_datetime(ds.time))\n",
    "        df = ds.isel(time=time_slice).to_dataframe().reset_index()\n",
    "        df[\"doy\"] = df[\"time\"].dt.day_of_year\n",
    "        df[\"sin_time\"] = np.sin(df[[\"doy\"]] / 360 * 2 * np.pi)\n",
    "        df[\"cos_time\"] = np.cos(df[[\"doy\"]] / 360 * 2 * np.pi)\n",
    "        self.x = {}\n",
    "        self.x[\"spatial\"] = df[[\"lat\", \"lon\"]].values\n",
    "        self.x[\"temporal\"] = df[[\"lat\", \"lon\"]].values\n",
    "        self.y = df[[variable]].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx : idx + 1], self.y[idx : idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sst_ds = SSTDataset(slice(60, 90), \"sst\", 123)\n",
    "sst_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SSTDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_slice,\n",
    "        variable,\n",
    "        seed=42,\n",
    "        num_workers: int = 0,\n",
    "        batch_size: int = 1028,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.time_slice = time_slice\n",
    "        self.variable = variable\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        dataset = SSTDataset(\n",
    "            time_slice=self.time_slice, variable=self.variable, seed=42\n",
    "        )\n",
    "        num_pts = len(dataset)\n",
    "        num_train = np.ceil(0.8 * num_pts)\n",
    "        num_valid = np.ceil(0.2 * num_pts)\n",
    "        self.train_set, self.valid_set = torch.utils.data.random_split(\n",
    "            dataset, lengths=[num_train, num_valid]\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return data.DataLoader(\n",
    "            dataset=self.val_set,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=numpy_collate,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return data.DataLoader(\n",
    "    #         dataset=self.test_set,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         collate_fn=numpy_collate,\n",
    "    #         num_workers=self.num_workers,\n",
    "    #         shuffle=False,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dm = SSTDataModule(slice(\"2013-01-01\", \"2013-02-01\"), \"sst\")\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jax_eo_py39]",
   "language": "python",
   "name": "conda-env-jax_eo_py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
